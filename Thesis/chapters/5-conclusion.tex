\label{Conclusion}

In the first part of the thesis, the key aspects of data, data quality and data quality tools were analyzed. A fundamental observation of data is that the high degree of diversity and complexity brings abundant data types and complex data structures, thereby increasing the difficulty of automatization in the data quality filed.\\
\hspace*{5mm} To the best of my knowledge, a new categorization of data quality tools was identified within the analysis of data quality tools. This categorization divides data quality tools into two categories, according to the target group of their users. The category of general-purpose data quality tools is aimed at less technical users, which can bring unsolicited complexity into its implementation.\\
\hspace*{5mm} The second category focuses primarily on data engineering teams that are expected to have higher technical knowledge. This new category of tools can avoid complex solutions due to omitting user features. A tool of this type offering a wide range of ML-based methods to support automation may present a potential void in the data quality tools market. Current general-purpose data quality tools do not take full advantage of the potential of ML-based methods.\\
\hspace*{5mm} The second part of the thesis deals with ML-based state-of-the-art methods which have the potential to address data measurement in an innovative way and are not part of the implementation of data quality tools.\\
\hspace*{5mm} Semantic data type detection (e.g. date, URL, country) can have a significant impact on the automation of ML-based methods. For example, a text feature could be adequately tokenized according to the semantic information contained in the value.\\
\hspace*{5mm} Automatically generating Regular Expressions via Genetic Programming has the potential to measure data quality in the field of automatic measurement of a number of wrong format records.\\
\hspace*{5mm} One of the most promising approaches to detecting text duplicates may be the development of a machine learning model with NLP approaches (Word2Vec, Bag-of-Words, TF-IDF or Fuzzy string matching).\\
\hspace*{5mm} An approach with elementary Autoencoder models was chosen in order to detect anomalies. An autoencoder model is trained for each dataset feature. This setting allows us to record the reconstruction error for the desired value in a feature as well as the total reconstruction error for an entire row. The input values of the autoencoder were tokenized text values converted to a numerical representation. This approach was able to detect approximately half of the manually created synthetic data quality defects on both datasets. Several potential data quality defects contained in the original dataset were found as well. The advantage of this approach is in its automation (only one essential parameter for defining the reconstruction error threshold is needed) and in its application on heterogeneous attributes. The alternative non-AI approach, where a regular expression was defined for format checking, has found all wrong records in a given data feature, but is task-dependent, requires expert knowledge and manual effort. As part of the extension of the Autoencoder approach, it would be appropriate to implement more advanced Autoencoders models (e.g. VAE) and additional metrics that would assess whether a given entire record is a suitable adept for data quality control as well as with other methods for semantic representation which would adequately represent the structure autoencoder input features. To the best of author's knowledge, this approach is not described in any existing literature nor it is implemented in existing data quality tools.\\
\hspace*{5mm} In the Association Rule Mining approach was chosen Apriori algorithm because of its ability to identify a set of underlying rules that collectively represent knowledge within input data. The NLP approach used in the experiment greatly facilitated the process of data preprocessing. This approach was able to extract business rules for a given business question but required preprocessing the data into the required transaction dataset. The complexity of data preprocessing for Association Mining algorithms may be one of the reasons why data quality tools do not widely support this approach. This approach seems challenging to automate. The similar alternative approach to the performed experiment is the manual definition of business rules which is entirely dependent on the skills of a domain expert.\\
\hspace*{5mm} Autoencoders and Association Rule Mining using NLP approaches were conducted as experiments on real-world, publicly available datasets in the practical part of the thesis. Both methods were compared with a similar alternative non-AI approach.\\
\hspace*{5mm} To conclude this thesis, a significant part of the thesis deals with data quality theory which provided a comprehensive insight into the field of data quality. The most significant contribution of this thesis is the finding of innovative approaches in the data quality measurement. Two of these state-of-the-art approaches were conducted as experiments.