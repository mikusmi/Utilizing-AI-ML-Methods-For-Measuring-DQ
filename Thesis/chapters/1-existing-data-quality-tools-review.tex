	\section{Existing data quality tools review}
	\label{sec:existing_data_quality_tools_review}

		Data quality tools are a vital data measure for the benefit of the business and should serve legitimate business needs \cite{McGilvray2008}. Due to urgent business requirements, the complexity of data \seesection{subsec:heterogeneity_in_data} and its storage space is increasing, putting pressure on efforts to care for their quality \cite{Chien2019}.

    	New features are emerging like automation, machine learning, business-centric workflows and cloud deployment models due to the strain on the market with data quality tools \cite{Chien2019}. According to a survey \cite{Ehrlinger2019} from 2019, 667 software tools dedicated to "data quality" were identified. The primary areas of these tools include data cleansing, data integration, master data management, and metadata management \cite{Greengard2019}. Based on demand, organizations prioritize the development of areas such as audience, governance, data diversity and latency \cite{Chien2019}. On the other hand, data quality tool vendors focus on enhancing areas such as analytics, intelligence, deployment and pricing \cite{Chien2019}. From the latest Gartner data available, the data quality tools market grew 11.6\% in the year 2017 \cite{Chien2019}.
    	
    	In lower abstraction, data quality tools eliminate data issues such as formatting errors, redundancy, inconsistency or wrong data types. They enable data errors and anomalies to be detected using algorithms and technologies (e.g. lookup tables) supporting automation of data quality processes \cite{Greengard2019}. The use of rules, automated processes and their detailed logging is considered as essential for supporting the data quality of an organization \cite{Greengard2019}. Current challenges for data quality tools include data consolidation associated with ETL tools, data validation reconciliation, data analytics, and managing large amounts of heterogeneous data \cite{Greengard2019}. One of the priorities of today's data quality tools is also to facilitate the work of data stewards \cite{Chien2019}.
    	
		The survey of data quality measurement and monitoring tools found the following findings based on measurements of 667 data quality tools in 2019:
		
		\begin{itemize}
			\item 50.82\% of data quality tools examined were domain-specific (dependence on data type or proprietary tool),
			\item 16.67\% of data quality tools focused on data cleansing without proper data quality measurement strategy,
			\item data profiling (to some extent) is the majority supported data quality technique with the potential extension to multi-column profiling and dependency discovery,
			\item no tools were found that implement a wide range of data quality metrics for the most important data quality dimensions \seesection{subsec:data_quality_dimensions_and_metrics},
			\item some data metrics implementations had implementation errors, some were applied to the attribute-level only (without aggregation), or required a potentially non-existent gold standard,
			\item more versatile data quality tools provide data quality monitoring as a paid premium feature,
			\item there are exceptions to open-source data quality monitoring tools, such as Apache Griffin or MobyDQ, which support rule automation but lack predefined functions and data profiling capabilities \cite{Ehrlinger2019}.
		\end{itemize}		    	
    	
		Users demand greater versatility of tools in terms of wider use of data management and information management capabilities \cite{Chien2019}. Therefore, a closer interaction of data quality tools with data integration tools, information governance frameworks and master data management (MDM) products is appreciated \cite{Chien2019}. Moving data quality tools to the cloud (SaaS for data quality) is becoming a modern trend \cite{Chien2019}.    		
    	It is important to realize that data quality tools can be useless and ineffective if a customer does not know what to want from such tools, what to expect from them and if the customer does not consider important prerequisite information such as how and where the organization store data, data flow, data usage and pricing models as well. Nowadays, it is not just a matter of the IT industry. It requires cooperation with executives and users \cite{Chien2019}. Thus, people with the appropriate knowledge of business needs, processes, and data work should cooperate closely with those who have the skills to implement and use the technology \cite{McGilvray2008}. It is also necessary to keep in mind the scenarios, which use multiple tools for solving data quality in the organization. Purchasing a data quality tool without knowing why and how a customer will use it -- the customer may get poor data quality or a low price-performance ratio of the data quality tool.  	
    	
    	\subsection{Selection criteria for data quality tool}
    	\label{subsec:selection_criteria_for_data_quality_tool}
    	
    		In the previous section, it has been emphasized several times that it is important to first understand the needs of data quality and then to find a suitable tool. This section discusses the criteria that play an essential role in the selection of a data quality tool. Knowledge of these criteria will help with the selection of relevant methods for research in the following section \seesection{ch:data_quality_measurement_methods}. First we look at the data quality criteria from a higher abstraction perspective and then move on to more detailed criteria.
													
			General data quality needs can be divided into three categories:			
				\begin{itemize}
					\item \textit{Data understanding} -- being aware of and comprehending the meaning, content, location and behaviour of data \cite{McGilvray2008}.
					\item \textit{Data repairing} -- correcting or updating data (closely related to phase Data Preparation in CRISP-DM ~process) \cite{McGilvray2008}.
					\item \textit{Data monitoring} -- checking data proactively by applying tests (such as quality control rules) that draw attention to anomalies in data to prevent business damage \cite{McGilvray2008}.
				\end{itemize}	
						
			Another view of selecting a data quality tool is as follows:	
				
			\begin{itemize}
				\item \textit{Identify your data challenges} -- Identify and analyze the data field of your organization. It is essential to analyze existing data sources, current tools in use, and data quality \cite{Greengard2019}.
				\item \textit{Understand what data quality tools can and cannot do} -- There is no data quality tool to repair completely broken, incomplete or missing data \cite{Greengard2019}. It is advisable to keep such limits in mind. The concept of GIGO (Garbage in, Garbage Out) also applies here, if the data framework of the organization suffers from fundamental deficiencies, these deficiencies are transferred to the tools of data quality and their potential will not be fully exploited.
				\item \textit{Understand the strengths and weaknesses of various data cleansing tools} -- Not all data quality management tools are equivalent \cite{Greengard2019}. Some are application-specific, such as Salesforce or SAP, others are focused on finding data quality issues in physical mailing addresses or e-mails, some dealing with IoT data \cite{Greengard2019}. It is necessary to understand the technical capabilities of the data cleansing tool (e.g. features, level of automation), the level of security and the pricing model of the tool \cite{Greengard2019}.
			\end{itemize}
	
				
    		Gartner publishes a more detailed list of key capabilities that organizations should consider when choosing a data quality tool:
    		 
    		\begin{itemize}
				\item \textit{Connectivity} -- Capability to access, and apply data quality rules to a wide range of data \seesection{subsec:heterogeneity_in_data} and data sources \seesection{subsec:diversity_of_data_stores} \cite{Chien2019}. 
				\item \textit{Data profiling, measurement and visualization} -- Capabilities to understand and analyse characteristics of varied types of data \seesection{subsec:types_of_data} and data issues to support data quality \cite{Chien2019}.
				\item \textit{Monitoring} -- Capabilities to assist users continuously respond to data quality issues, monitor data and assurance data quality \cite{Chien2019}.
				\item \textit{Parsing} -- Capabilities to decompose data into its component parts based on the need to better understand the internal characteristics of data \cite{Chien2019}.
				\item \textit{Standardization and cleansing} -- Capabilities to standardize (e.g. industry or local standards, business rules, knowledge bases) and cleanse the data to achieve specific formats, values and layouts \cite{Chien2019}.
				\item \textit{Matching, linking and merging} -- Capabilities for matching, linking and merging related data entries within or across datasets via a variety of techniques, such as rules, algorithms, metadata and machine learning \cite{Chien2019}.
				\item \textit{Multidomain support} -- The capability to support a specific area of data such as customer, product, property and location to ensure sufficient quality \cite{Chien2019}.
				\item \textit{Address validation/geocoding and other validation} -- Support for location-related data standardization and cleansing \cite{Chien2019}.
				\item \textit{Data curation and enrichment} -- The capability to integrate externally sourced third-party data to improve completeness and add value (such as Demographic Data Enrichment, Geographic Data Enrichment)\cite{Chien2019}.
				\item \textit{Issue resolution and workflow} -- Capabilities to correctly set the process flow and user interface that enables business users to identify, quarantine, assign, escalate and resolve data quality issues \cite{Chien2019}.
				\item \textit{Metadata management} -- The capability to capture, reconcile and interoperate metadata relating to the data quality process to promote greater confidence in data \cite{Chien2019}. Indication of how and when the data got to the system and what business rules were applied is important.
				\item \textit{DevOps environment} -- Capabilities that facilitate configuration and application of data quality operations \cite{Chien2019}. Such as capabilities of Data DevOps that can give increased speed, quality, security, and a great degree of productivity \cite{Smith2019}.
				\item \textit{Deployment environment} -- The capability to deploy data quality operations to the organization's hardware and software \cite{Chien2019}.
				\item \textit{Architecture and integration} -- The capability of commonality, consistency and interoperability among various components of data quality tools environment and third-party tools \cite{Chien2019}.
				\item \textit{Usability} -- The capability to maximize the usability of a data quality tool to the benefit of an organization's business \cite{Chien2019}.
			\end{itemize}			    	
    	
			Based on the above criteria, which should be taken into account when selecting a data quality tool, we can conclude that the effort and time investment in analysing the implementation of a data quality environment into the organization is essential.    	
    	
		\subsection{Data quality tools analysis}    	
    	
    		Healthy competition in the market for data quality tools shows their importance. There are a plethora of these tools, so a fundamental analysis of existing data quality tools \seeapendixfile{DQ-tools-analysis.xlsx} was carried out in the context of the previous findings. Specifically, the analysis is a search of 21 data quality tools and their parameters (such as focus, key features, documentation, support, pricing model, open/close source code, strengths and cautions).
    		
      		The aim was to obtain basic information about the approach to data quality of these tools, so that the selection of methods for analysis in the following sections is as close as possible to the methods used in practice. Learning from the tools shortcomings is one of the key priorities of this analysis as well.
      
      		Information was drawn from publicly available information on the web, studies and articles. The practical analysis was omitted as it is a time-consuming activity outside the scope of this work. A valuable study for analysis was "Magic Quadrant for Data Quality Tools 2019" \cite{Chien2019} from Gartner Inc., which includes vendors offering commercial software tools or cloud-based services with data quality features (profiling, parsing, standardization/cleaning, matching, monitoring). The study is focused on the data quality tools market than on functionality itself. Therefore, a survey \cite{Ehrlinger2019} of 13 free/trial and domain-independent data quality software tools was included in the analysis.
      		
      		The main findings of the analysis are as follows. Due to the high degree of diversity and complexity in the data, the existence of a unitary and universal tool is doubtful. Therefore, vendors limit data quality tools to specific domains (e.g. finance), technologies (e.g. SAP), data types (e.g. structured data) and data processing (e.g. batch processing) to reduce the complexity of systems.
      		
      		It seems that maintaining universality is proportional to the cost. This phenomenon can be seen with data quality tools such as SAP, SAS and IBM, which are leaders in the industry \cite{Chien2019}. These tools cover a wide range of data quality methods, technologies and support functions, but such scope negatively affects the cost of these tools. Over the years, the complexity and interconnectivity of these tools have increased. It leads to the tendency of vendor-locking \cite{Chien2019}. This lock may not only be a purely business intent, but also a limited ability to integrate a complex system to third-party tools. Thus, vendors create a single data quality environment. By purchasing one tool, the customer is pushed in this vendor environment to stay. The environment of vendors is, in some cases, reminiscent of the financial or banking sector, which can be harmful and distract vendors' attention in the wrong technical direction.
      		
      		Vendors know about the complexity of the data quality systems. Therefore, efforts are being made to move data quality tools to a single cloud environment (e.g. by combining data integration, data preparation, and stewardship) to be served by all data quality actors in one place \cite{McDaniel2019}. Data quality intervenes in various roles that need close cooperation. Such tools include Talend Data Management Platform\footnote{\url{https://www.talend.com/products/data-integration/data-management-platform/}} or SAS Viya\footnote{\url{https://www.sas.com/en_us/software/viya.html}}. This step is mainly due to the growth of cloud data storage industry and an increase in demand for cloud integration solutions (e.g. Talend Cloud Integration Platform\footnote{\url{https://www.talend.com/products/integration-cloud/}}). Data quality tools are closely linked to these data integration tools and should be given due consideration \cite{McDaniel2019}. If the data progresses to higher levels of the system, the probability of errors or deficiencies in the data increases, so practising for an active approach to data quality is suitable prevention. This preventive behaviour allows to check and measure the level of quality before it even really gets into the core systems \cite{McDaniel2019}.
      		
      		Nowadays, the ability to service data quality of IoT and mobile devices is essential to data quality tools \cite{Pearlman2019a}. It is a challenge of unifying and consolidating a wide range of data formats from multiple data streams and control their quality ("real-time data quality") \cite{Pearlman2019a}. In terms of innovation, the use of artificial intelligence and machine learning technologies to simplify and automate processes (automated project configuration, automated metadata discovery, anomaly detection, evaluation of results, etc.) is an essential approach of the data quality vendors leaders \cite{Chien2019}.
      		
      		Users consider important to have professional technical support \cite{Chien2019}. It should be noted that the support is closely related to the price of the data quality tool. For this reason, it is worth investing in the documentation, simplifying the process of deploying and integrating the tool, supporting the community, and the ease of use of the tool.
      		
      		From the analyzed tools, three "providers" were selected that stand out -- Talend\footnote{\url{https://www.talend.com/}}, Informatica\footnote{\url{https://www.informatica.com/}} and MobyDQ\footnote{\url{https://github.com/ubisoftinc/mobydq}}.
      		
      		Data quality tools characterized by ease of use and an active open-source user community include Talend Open Studio (TOS) for Data Quality (and a free open source license) and Talend Data Management Platform (a user-based subscription)\cite{Ehrlinger2019}. The Talend has doubled the number of customers with data quality licenses between 2017 and 2018 \cite{Chien2019}. It is a relatively young company (founded 2005) with a higher degree of adaptability. On the other hand, the tool is criticized for the lack of technical support, the difficulty of upgrading and migrating data between versions, and the lack of monitoring, reporting and scheduling features for data quality processes \cite{Chien2019}. Talend Open Studio for Data Quality is the leading open-source data profiling tool \cite{Ehrlinger2019}.
      		
      		In contrast, Informatica has been on the market since 1993 and is considered the "gold standard" among data quality tool providers \cite{Chien2019}. Some users considered that its prices are high and the licensing models are complicated, but the company responded quickly and introduced more dynamic models such as pay-as-you-go \cite{Chien2019}. It should be noted that it is a sophisticated tool, and it is necessary to consider the financial return of such investment when buying it. Informatica is one of the most versatile and innovative tools -- introduces innovations mainly in the field of machine learning such as metadata-driven machine learning to identify data domain consistency, outliers and errors, and range of tasks related to Internet of Things (IoT), MDM, data governance and content-driven analytics \cite{Chien2019}. It has a robust global ecosystem, with over 500 partners for all of its products, including Accenture, Amazon, Cognizant, Deloitte, Google and Microsoft \cite{Chien2019}. On the other hand, users report that the initial setup is complex, time consuming to teach users to use tools, poor stability (restart the server at least once a month), and outdated visualization capabilities \cite{Chien2019}.
      		
%			\todo[inline]{[Book] Data quality dimensions - see Chrome Bookmarks in Book folder - Butterfly effect}      		
      		
			A new categorization of data quality tools was observed from the analysis. The above-mentioned tools (mostly general-purpose tools) are, in some ways, aimed at less technical users, which can bring unsolicited complexity into its implementation. If we focus primarily on data engineering teams that are expected to have the higher technical knowledge, we can avoid complex solutions due to omitting user features. And bring an active approach to data quality closer to the data source \cite{McDaniel2019}. This avoids the likelihood that poor quality data will bubble through the system to higher layers where it can potentially cause more serious damage. Detecting the source of errors in data within a long data footprint is costly and time-consuming. These types of tools are Apache Griffin\footnote{\url{https://github.com/apache/griffin}} and MobyDQ. Let's take a closer look at MobyDQ.	      
      
			"MobyDQ is a tool for data engineering teams to automate data quality checks on their data pipeline, capture data quality issues and trigger alerts in case of anomaly, regardless of the data sources they use." \cite{Rolland2020}  It is an open source project inspired by an internal data quality project developed by Ubisoft Entertainment \cite{Rolland2020}. In contrast to Apache Griffin, MobyDQ could be installed quickly and straightforward, based on detailed documentation provided on GitHub \cite{Ehrlinger2019}. On the other hand, MobyDQ does not provide any data profiling functionality \cite{Rolland2020}. A tool of this type offering a wide range of ML-based methods to support automation may present a potential void in the data quality tools market. 
			